    
\documentclass{article} %
\usepackage{iclr2026_conference}
\usepackage{times}

\input{math_commands.tex}

\usepackage{hyperref}
\usepackage[capitalise]{cleveref}
\usepackage{amsthm}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{cancel}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}


\usepackage{pifont}
\definecolor{myred}{RGB}{215,48,39}
\definecolor{mygreen}{RGB}{26,152,80}
\newcommand{\yes}{\textcolor{mygreen}{\ding{51}}}
\newcommand{\no}{\textcolor{myred}{\ding{55}}}

\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\definecolor{codecomment}{rgb}{0.25,0.5,0.35}
\definecolor{codestring}{rgb}{0.6,0.1,0.1}
\definecolor{codekeyword}{rgb}{0.1,0.1,0.7}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{codebg},
    commentstyle=\color{codecomment}\itshape,
    keywordstyle=\color{codekeyword}\bfseries,
    stringstyle=\color{codestring},
    basicstyle=\ttfamily\small,
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    rulecolor=\color{gray},
    tabsize=4,
    captionpos=b
}
\lstset{style=mystyle}

\renewcommand{\cite}{\citep}
\newcommand{\Uu}{\mathcal{U}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}

\DeclareMathOperator{\Pick}{Pick}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\scal}[2]{\langle #1, #2 \rangle}
\newcommand{\KL}[2]{\mathrm{KL}(#1 \,\|\, #2)}

\newcommand{\correct}{\ensuremath{\#\text{correct}}}
\newcommand{\accepted}{\ensuremath{\#\text{accepted}}}

\newtheorem{defi}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}


\title{Parallel Token Generation \\ for Language Models}


\author{Felix Draxler\textnormal{$^\ast$} \quad Justus Will\thanks{Equal contribution. Corresponding authors: \texttt{\{fdraxler,jcwill,mandt\}@uci.edu}}\,\,\, \quad Farrin Marouf Sofian \\
Department of Computer Science, \\
University of California, Irvine \\
\And
Theofanis Karaletsos \\
Chan-Zuckerberg Initiative \\
\& Pyramidal AI \\
\And
Sameer Singh \\
Department of Computer Science, \\ 
University of California, Irvine \\
\And
Stephan Mandt \\
Computer Science \& AI in Science Institute, \\ 
University of California, Irvine \\
}




\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\iclrfinalcopy %
\begin{document}


\maketitle


\begin{abstract}
    We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.
\end{abstract}


\section{Introduction}

Autoregressive transformers \citep{vaswani2017attention} are the foundation of today’s large language models (LLMs) \citep{brown2020language}. Their sequential generation process, however, remains a major bottleneck: each token depends on the full history, requiring one forward pass per token. For long outputs, this increases the inference latency significantly compared to what a single transformer call would achieve.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figs/prompt_comparison.pdf}
    \caption{\textbf{Our parallelized model generates the same text as its teacher in a fraction of the steps.} By the time our model \textit{(bottom)} has generated an entire function, an autoregressive model \textit{(top)} only generates the method's signature. Prompt: \texttt{Write a Python function that computes the factorial of a number}. Green tokens are accepted tokens in that step, red tokens are incorrect. Semitransparent tokens are rejected after the first mistake.}
    \label{fig:code-sampling}
\end{figure}


Many recent efforts aim to bypass this bottleneck by predicting multiple tokens at once. Broadly, they can be categorized into two lines of work: the first, speculative decoding, takes a systems approach, making predictions in a lightweight model that is verified by a large model \citep{leviathan2023fast,chen2023accelerating,sun2023spectr,zhong2025speeding}. The second line of work makes use of predicting several tokens independent of each other. This significantly reduces the search space for sequences and improves overall model quality \citep{qi2020prophetnet,gloeckle2024better,deepseek-ai2025deepseekv3}. Similarly, discrete diffusion iteratively refines generated sequences, again not modeling conditional dependencies between tokens in each denoising step \citep{hoogeboom2021argmax,austin2021structured}. However, all of these methods still contain an irreducible sequential component to generate sequences.

Our work takes a step towards filling this gap. We propose a framework that, in theory, can generate arbitrary length sequences in parallel. This is enabled by a small but fundamental architectural change: instead of sampling from the distributions predicted by an autoregressive model in a post-processing step, we feed the involved random variables as an input to the model: the model learns to sample. This enables it to anticipate which tokens will be sampled and predict them jointly.
Similar frameworks have been formulated in the normalizing flow literature: Inverse Autoregressive Flows \citep{kingma2016improved} generate samples of many continuous dimensions in parallel and Free-form Flows \citep{draxler2024freeform} distill a fast generator network. We transfer these concepts to sampling discrete sequences from a continuous latent space.

Our contributions are as follows:
\begin{itemize}
    \item We propose \textit{Parallel Token Prediction} (PTP), a modeling approach for discrete data that generates multiple interdependent tokens in one model call~(\cref{sec:parallel-sampling}).
    \item We prove that PTP is as expressive as autoregressive models~(\Cref{thm:one-hot-PTP,thm:C-PTP}).
    \item PTP can be trained to predict several tokens either by distilling an existing teacher~(\cref{sec:distillation}), effectively parallelizing it, or from scratch on training data~(\cref{sec:autoregressive-training}).
    \item Experimentally, we train real-world coding and natural language PTP models, achieving 7.0~respectively 4.18~accepted tokens per speculative decoding step~(\cref{sec:experiments}).
\end{itemize}
Together, our framework opens a design space to build models that accurately predict several tokens in parallel, reducing latency in language model output without limiting representational power.


\section{Parallel Token Prediction}

\subsection{Parallel Sampling}
\label{sec:parallel-sampling}


To construct our \emph{Parallel Token Prediction} framework, let us recap how a classical transformer decoder generates text. It iteratively predicts the categorical distribution of the next token $t_i \in \{1, \dots, V\}$ based on all previous tokens $t_{<i} = (t_1, \dots, t_{i-1})$,
\begin{equation}
    P_i := P(t_i|t_{<i})
    \label{eq:autoregressive-model}
\end{equation}
For simplicity, we assume this distribution is the final distribution that is used to generate tokens, in that it already reflects temperature scaling \cite{guo2017calibration}, top-k and top-p sampling \cite{holtzman2020curious}, or other approaches trading sample diversity with quality.
To sample a token from this distribution, one draws an auxiliary random variable $u_i \sim \Uu[0, 1]$ and looks up the corresponding token from the cumulative distribution function as follows:
\begin{equation}
    t_i = \operatorname{Pick}(u_i, P_i) \equiv \min_{j \in \{1, \dots V\}} \{ j: F_{ij}> u_i \},\quad\text{where } F_{ij} = \sum_{l=1}^{j} P_{il}.
    \label{eq:autoregressive-sampling}
\end{equation}
Here, $j$ iterates possible token choices, $P_{il}$ is the probability to sample $t_i = l$, and $F_{ij}$ is the cumulative distribution to sample a token $t_i \in \{1, \dots j\}$.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figs/sampling-comparison-4.pdf}
    \caption{\textbf{Parallel Token Prediction predicts several tokens in one model call.} (a)~An autoregressive model predicts the distribution of the single next token $t_i$. From the histogram, that token is chosen with the help of an auxiliary variable $u_i$ that is chosen uniformly at random (see \cref{fig:pick-token-function}). (b)~One-Hot Parallel Token Prediction merges the sampling into the model by feeding the auxiliaries directly into the model. This allows joint prediction of several tokens. (c)~Categorical Parallel Token Prediction models the distribution of each token, but predicts them in parallel using the auxiliary variables.}
    \label{fig:sampling-comparison}
\end{figure}

\Cref{fig:sampling-comparison}(a) illustrates how, in traditional autoregressive models, we first sample $t_i$ from $P_i$ before moving on to predicting the next token $t_{i+1}$, as the distribution $P_{i+1}$ depends on the selected token $t_i$. Every new token involves another model call, increasing latency.
To break this iterative nature, note that while \cref{eq:autoregressive-model} defines a distribution over possible next tokens, \cref{eq:autoregressive-sampling} is a deterministic rule once the auxiliary variable $u_i$ is drawn. Thus, write this rule as an explicit deterministic function:
\begin{equation}
    \label{eq:deterministic-token-given-auxiliary}
    t_i = f_P(t_{<i}; u_i) = \Pick(u_i, P(\,\cdot\, |t_{< i})).
\end{equation}
\Cref{fig:pick-token-function} illustrates how this function jumps from token to token as a function of $u_i$.
\begin{wrapfigure}{r}{0.45\textwidth}
    \centering
    \includegraphics[width=\linewidth]{figs/pick-token-function-truncated.pdf}
    \caption{\textbf{Sampling from a discrete distribution.} Given a histogram $P_i$ \textit{(left)}, compute the inverse cumulative distribution function \textit{(right)} and look up the token at a random location $u_i \in \Uu[0, 1]$. Our framework relies on considering both parts jointly.}
    \label{fig:pick-token-function}
\end{wrapfigure}

These auxiliary variables are all we need to perform parallel generation of text: all information about which token $t_i$ we are going to select is available to the model if it has access to $u_i$ as one of its inputs. By repeating the above argument and feeding all the auxiliary variables into the model, any subsequent token $t_{>i}$ can be predicted deterministically (proof in \cref{sec:proof-one-hot-PTP}):
\begin{theorem}
    \label{thm:one-hot-PTP}
    Let $P$ denote a probability distribution for next token prediction. Then, the future token $t_k$ can be selected as a deterministic function $f_P$ of previous tokens $t_{<i}$ and auxiliary variables $u_i, \dots, u_{k} \sim \mathcal{U}[0, 1]$:
    \begin{equation}
        t_{k} = f_P(t_{<i}; u_i, \dots u_{k}), \qquad \text{for all }k \geq i.
        \label{eq:one-hot-PTP}
    \end{equation}
\end{theorem}
\Cref{thm:one-hot-PTP} shows a clear path to build a model that can sample many tokens in parallel: instead of learning the distribution $P(t_k|t_{<k})$, we propose to directly fit the function $f_P(t_{<i}; u_i, \dots, u_{k})$, which jointly predicts future tokens $t_k$.

\Cref{fig:sampling-comparison}(b) visualizes how this path can be implemented with a standard transformer \cite{vaswani2017attention} backbone: alongside the previous tokens, simply feed the auxiliary random variables for the next $N$ tokens into the model. Then predict $P(t_k|t_{<i}; u_i, \dots, u_k)$ for each future token. Per \cref{thm:one-hot-PTP}, this prediction narrows down to a single token: $P(t_k|t_{<i}; u_i, \dots, u_k) = \mathbf{1}(t_k = f_P(t_{<i}; u_i, \dots, u_k))$. In practice, we take the $\mathrm{argmax}$ to extract that predicted token token:
\begin{equation}
    \label{eq:o-ptp-sampling}
    t_k = f_P^{\text{O-PTP}}(t_{<i}; u_i, \dots, u_k) = \operatorname{argmax}(P(t_k \mid t_{<i}; u_i, \dots, u_k))
\end{equation}
We refer to this model as a \textbf{One-Hot Parallel Token Prediction Model}~(O-PTP). O-PTPs can be trained to replicate an existing autoregressive model $P$, as we discuss in \cref{sec:distillation}.

O-PTPs converge to one-hot distributions, and therefore do not expose the underlying sampling distributions of the model. This prevents access to the original conditional probabilities $P(t_k|t_{<k})$, which are required for training without a teacher, adjusting temperature \cite{holtzman2020curious}, and uncertainty quantification. To address this, we introduce \textbf{Categorical Parallel Token Prediction}~(C-PTP), which recovers the full conditional distribution of each token. The key idea is to predict each token $t_k$ while conditioning on all \emph{past} auxiliary variables $u_i, \dots, u_{k-1}$, but explicitly excluding its own auxiliary variable $u_k$. By \cref{thm:one-hot-PTP}, these past auxiliaries deterministically encode the sampled history $t_{<k}$. Withholding $u_k$ preserves the uncertainty over $t_k$ rather than collapsing it to a point mass. As a result, conditioning $t_k$ on $(t_{<i}, u_i, \dots, u_{k-1})$ exactly recovers the original autoregressive conditional $P(t_k \mid t_{<k})$, as formalized below.
\begin{theorem}
    \label{thm:C-PTP}
    Let $P$ denote a probability distribution for next token prediction. Then, the distribution of a token $t_{k}$ is fully determined by context tokens $t_{<i}$ and the past auxiliary variables $u_{i}, \dots, u_{k - 1}$:
    \begin{equation}
        \label{eq:C-PTP}
        P(t_{k}|t_{<i}, u_i, \dots, u_{k - 1}) = P(t_{k}|t_{<k}), \qquad \text{for all }k \geq i.
    \end{equation}
    Notably, \cref{eq:C-PTP} does not depend on $u_k$.
\end{theorem}
\Cref{fig:sampling-comparison}(c) shows how \cref{thm:C-PTP} can be used to predict the distribution of the tokens $t_{i}, \dots t_N$ in parallel. Just like for O-PTP, first sample all required auxiliary variables $u_{i}, \dots u_{N}$, and then predict all $P_k = P(t_{k}|t_{<i}, u_i, \dots, u_{k - 1})$ in parallel. Sampling from these distributions is done just as in \cref{eq:deterministic-token-given-auxiliary}:
\begin{equation}
    \label{eq:c-ptp-sampling}
    t_k = f_{P}^{\text{C-PTP}}(t_{<i}, u_i, \dots, u_{k}) = \Pick(u_k; P(t_{<i}, u_i, \dots, u_{k-1})).
\end{equation}
By using a causal decoder architecture, we can properly mask which token has access to which auxiliaries.

C-PTP can be trained without a teacher by iteratively solving \cref{eq:C-PTP} for the auxiliary $u_k$ corresponding to a given token $t_k$, see \cref{sec:autoregressive-training}. Like O-PTP, it can also be distilled from a teacher.

We provide a full proof for \cref{thm:C-PTP} in \cref{sec:proof-C-PTP} and give a short intuition here:
For the first token, \cref{eq:C-PTP} is identical to the original autoregressive distribution in \cref{eq:autoregressive-model}:
    $P_i = P(t_{i}|t_{<i}, \cancel{u_i}) = P(t_{i}|t_{<i})$.
Moving to the next token $t_{i+1}$, we now do pass in the auxiliary variable $u_i$ used to sample the first token $t_i$. Since $P_i$ and $u_i$ uniquely determine $t_i$, $u_i$ and $t_i$ contain the same information. By the law of total probability, this recovers \cref{eq:autoregressive-model} for $t_{i+1}$:
    $P_{i+1} = P(t_{i}|t_{<i}, u_i) = P(t_{i+1}|t_{<i}, t_i).$
Repeating this argument, we find that the distribution of every future tokens is available if we condition on all preceding auxiliary variables, concluding the proof. %

Both One-Hot and Categorical Parallel Token Prediction allow coordinated token prediction in a single model call. \textbf{By \Cref{thm:one-hot-PTP,thm:C-PTP}, there are no fundamental restrictions as to how many tokens can be jointly modeled} apart from model capacity. In the next section, we propose two approaches to train from scratch (only C-PTP) or by distillation of an existing model (both O-PTP and C-PTP).



\subsection{Training}
\label{sec:training}


Before deriving the training paradigms for Parallel Token Prediction Models, let us quickly recall that autoregressive models are trained by minimizing the cross-entropy between samples from the training data $t \sim P(t)$ and the model $P_\theta$
\begin{align}
\label{eq:ce}
    \mathcal{L}(\theta) = \EE_{t \sim P(t)}\!\left[- \sum_{i=1}^N \log P_\theta(t_i \mid t_{1\dots i-1})\right].
\end{align}
Using a causal model such as a transformer \cite{vaswani2017attention} this loss can be evaluated on an entire sequence of tokens in a single model call \cite{radford2018improving}.
We first present how to distill both One-Hot and Categorical Parallel Token Prediction Models from a trained autoregressive model.
We then show how the latter can be self-distilled from data alone via \cref{eq:ce}.


\subsubsection{Distillation}
\label{sec:distillation}

Both PTP variants can be trained to emulate the token-level predictions of an autoregressive teacher $Q_\varphi$, allowing for efficient, parallel generation of several tokens. We then call the PTP a student model $P_\theta$. With enough data and model capacity, our algorithm leads to a student model that produces the same sequence of tokens in a single model call as the teacher does in high-latency autoregressive sampling (\Cref{thm:one-hot-PTP,thm:C-PTP}). We defer correcting errors arising from finite resources to the subsequent \cref{sec:error-correction}.


To train the student for a given training sequence $t_{1}, \dots, t_{T}$, we reverse engineer the auxiliary variables $u_1, \dots, u_N$ under which the teacher would have generated it, split the sequence into context and prediction sequences, and then evaluate a loss that leads the student towards the correct generation. This process is summarized in \cref{alg:ptp-distillation} in \cref{sec:algorithms}.

\textbf{Auxiliary variables.} First, we extract the auxiliary variables that the teacher model would use to generate the training sequence. We evaluate the teacher distributions of each training token to get the cumulative discrete distributions $F_1, \dots, F_T$ for each token. Inverting \cref{eq:autoregressive-sampling}, we find for every $k = 1, \dots, T$:
\begin{equation}
    u_{k} \in [F_{k, t_{k}-1}, F_{k, t_{k}}).
    \label{eq:compatible-auxiliaries}
\end{equation}
Since $u_{k}$ is continuous, while $t_{k}$ is discrete, we can randomly pick any compatible value. See \cref{sec:beta} for details.

\textbf{Sequence splitting.} Second, we split the training sequence into a context part $t_1, \dots, t_{i-1}$ and a prediction part $t_{i}, \dots, t_T$. We usually choose the split index $i$ at random and then predict a fixed number of parallel tokens.

\textbf{Loss evaluation.} Both O-PTP and C-PTP can be trained with cross-entropy given an input sequence $t$ and a split index $i$, with $u_k$ extracted using \cref{eq:compatible-auxiliaries}:
\begin{align}
    \label{eq:o-ptp-cross-entropy}
    \text{O-PTP:}&\quad\mathcal{L}(\theta; t, i) = - \sum_{k=i}^{T} \log P_\theta(t_{k}|t_{<i}, u_{i}, \dots, u_{k-1}, u_{ k}), \\
    \label{eq:c-ptp-cross-entropy}
    \text{C-PTP:}&\quad\mathcal{L}(\theta; t, i) = - \sum_{k=i}^{T} \log P_\theta(t_{k}|t_{<i}, u_{i}, \dots, u_{k-1}).
\intertext{C-PTP can also be trained similar to knowledge distillation \cite{hinton2015distilling}: to this end, we explicitly match the student's $P_{\theta, k}$ to the teacher distribution $Q_{\varphi, k}$. This works for any loss $d(Q, P)$ for the difference between categorical distributions, such as the Kullback–Leibler divergence $d = \KL{Q}{P}$ or its reverse variant $d = \KL{P}{Q}$:}
    &\quad\mathcal{L}(\theta; t, i)
    = \sum_{k=i}^{T} d(Q_\varphi(t_{k}|t_{<k}), P_\theta(t_{k}|t_{<i}, u_{i \dots k-1})).
    \label{eq:c-ptp-divergence}
\end{align}
Note that while different losses $d$ have different convergence properties, crucially, $d = 0$ implies identical conditional distributions and a perfectly distilled model.


\textbf{Training data.} We can train the student using sequences from any data source, even if the teacher assigns different probabilities to them. As long as the teacher assigns non-zero probability and the dataset is sufficiently large, querying the teacher for the corresponding auxiliary variables provides full supervision and allows the student to match the teacher everywhere.

This gives us a large design space and we compare several options empirically in \cref{sec:losses}.  If our goal is to deploy our parallelized student as a drop-in replacement of our teacher model, the lowest-variance option is to sample training sequences from the teacher.
Another possibility is to directly sample training sequences from a dataset, such as the one that was used to train the teacher model in the first place.
This has the advantage that we can compute the teacher predictions $Q_\varphi(t_k \mid t_{<k})$ in parallel over a full sequence instead of iteratively having to generate it.
Finally, we can sample sequences directly from the student model by first sampling auxiliary variables $u_i, \dots, u_T \sim \mathcal{U}[0, 1]$ and then using our student model at its current state to sample training sequences in parallel. As the student's prediction gets closer to that of the teacher during training, this approaches the same training sequence distribution as if we had sampled the teacher directly. %

Conceptually, the latter is similar to the techniques used in distilling the autoregressive WaveNet into a parallel model \cite{oord2018parallel}. In practice, following their approach, which equates to using a reverse KL loss as noted above, is efficient but proved to be too unstable. Parallel WaveNet stabilizes training with several auxillary losses, like a perceptual loss, which we don't have in our setting, in general.


\subsubsection{Inverse Autoregressive Training}
\label{sec:autoregressive-training}

Categorical Parallel Token Prediction Models can also be trained directly via \cref{eq:ce}, avoiding the need to have a teacher model as target. For a given training sequence $t_{1}, \dots, t_{T}$, we again split it into the context $t_{<i}$ and the following prediction $t_{\geq i}$. %

Exactly as for distillation, we have to find auxiliary variables that are compatible with every $t_k, k \geq i$. We can do this by selecting, randomly, any
 $   u_{k} \in [F_{k, t_{k}-1}, F_{k, t_{k}})$,
equivalently to \cref{eq:compatible-auxiliaries}, where $F_{k}, t_{k}$ now is the cumulative probability under $P_\theta$ (instead of the teacher model) to choose $t_k$ when predicting that token.
As this probability depends on the previous auxiliary variables $u_{i}, \dots, u_{k-1}$, we select them iteratively.
Specifically, we can alternate between computing the logits of $P_\theta(t_k \mid t_{<i}, u_{i} , \dots, u_{k-1})$, and drawing $u_k$ using \eqref{eq:compatible-auxiliaries}. 

Finally, we can train our model using the cross-entropy loss
\begin{align}
\label{eq:ce-iat}
    \mathcal{L}(\theta) = \EE_{t \sim P(t), i \sim P(i|t)}\!\left[- \sum_{k=i}^{N} \log P_\theta(t_{k}|t_{<i}, u_{i}, \dots, u_{k-1})\right].
\end{align}
\Cref{alg:ptp-inverse-autoregressive} in \cref{sec:algorithms} summarizes the procedure. A similar approach of iteratively determining latent variables (our auxiliaries) was proposed by Inverse Autoregressive Flows~\cite{kingma2016improved}, although they considered continuous variables that are traced through an invertible neural network.


\subsection{Error Correction}
\label{sec:error-correction}

\Cref{thm:one-hot-PTP,thm:C-PTP} say that with enough model capacity, O-PTP and C-PTP can correctly predict coherent sequences of arbitrary length in one model call. Practically, finite model capacity limits the length at which a single transformer pass can produce coordinated text. In this section, we present block-autoregressive approaches that speed up token generation through parallel prediction while maintaining a identical output. This combines ideas from speculative decoding \cite{leviathan2023fast} with the PTP setups described in \cref{sec:parallel-sampling}.

Intuitively, we only accept a sequence of tokens if it could have been generated exactly by a high-quality autoregressive model. In distillation settings, the autoregressive teacher serves as a verifier. When no teacher is available, we instead verify the sequence using the model itself, while never using more than one auxiliary variable at a time.



We begin by fixing a sequence of auxiliary variables $u_i, \dots, u_N$, which fully determine a sample. Conditioned on these auxiliaries, PTP $P_\theta$ generates an entire token sequence $t_i, \dots, t_N$ from the context $t_{<i}$ in a single forward pass (cf.~\cref{eq:o-ptp-sampling,eq:c-ptp-sampling}).

To verify that this sequence is consistent with the autoregressive reference model $Q_\varphi$, we run that model in parallel on the generated tokens. For each position $k$, we check whether sampling from the reference model $Q_\varphi(t_k \mid t_{<k})$ using the same auxiliary variable $u_k$ (as in \cref{eq:autoregressive-sampling}) would produce the same token as PTP. We accept all leading tokens for which this condition holds, and additionally accept one more token from the teacher at the first position where the models disagree.
This is made explicit in \cref{alg:error-correction} in \cref{sec:algorithms}.






We define the \textit{number of correct tokens} \correct{} by counting the identically predicted tokens. Similarly, the \textit{number of accepted tokens} also includes the token corrected by the teacher, so usually \accepted{} = \correct{} $+ 1$. See \cref{accepted-tokens-metric} for formal definitions.



If both PTP and the verification model have the same size, and we execute the two models naively in sequence, this results in a wall-time speedup of: $\accepted / 2 = (\correct + 1) / 2$. In \cref{sec:speculative-decoding} we will see that smaller model sizes further increase the speedup.
We discuss how to leverage additional computing resources to further decrease the total number of sequential model calls and avoid the overhead of verification in \cref{sec:more_gpu}.


\section{Experiments}
\label{sec:experiments}

We empirically evaluate Parallel Token Prediction (PTP) using the number of accepted tokens per step under teacher verification (\accepted{}, see \cref{sec:error-correction}) as this is directly proportional to the maximum speedup that can be obtained with efficient decoding \cite{samragh2025your}. We first show that C-PTP can be trained from data alone without access to a teacher model~(\cref{sec:inverse-autoregressive-experiment}). We then distill a 1.1B-parameter model on code generation and demonstrate that O-PTP achieves larger speedups than autoregressive speculative decoding by enabling the draft model to predict multiple tokens per call~(\cref{sec:speculative-decoding}). On the same task, we show that conditioning on auxiliary variables yields substantially more correct tokens than independently predicting multiple tokens~(\cref{sec:independent-failure}). Finally, on a speculative decoding benchmark spanning diverse language tasks, we finetune a 7B-parameter model and show that O-PTP consistently outperforms competitive baselines, achieving state-of-the-art parallel decoding performance~(\cref{sec:vicuna}).



 



\subsection{Design Choices}
\label{sec:ablation}

We implement PTP as a lightweight modification of a standard transformer architecture by adding a few parameters for the embedding of auxiliary variables. Specifically, we adopt the following approach:
\begin{equation}
    \operatorname{embed}(u) = W \operatorname{bits}(u) + b, \quad\text{where }\operatorname{bits}(u) \in \{0, 1\}^{32}.
\end{equation}
Here, $\operatorname{bits}(u)$ maps the IEEE-754 bits of the \texttt{float32} number $u$ into a 32-dimensional vector, resembling an arithmetic coding scheme \cite{witten1987arithmetic}. The linear layer $(W, b)$ maps this binary vector to the embedding space of the autoregressive transformer. This embedding is then added to the positional encoding of the underlying network.
We present the reasoning behind this choice through an ablation in \cref{app:ablation}. It is performed on a dataset that predicts pick-up locations for taxis in New York City (NYC TLC, \citeyear{nyc_tlc_2016}). 

For distilling a fast student model from a teacher, we train O-PTP with cross-entropy (\cref{eq:o-ptp-cross-entropy}), sample training data from the teacher once and train multiple epochs on it. We again identify these choices through an ablation and practical considerations in \cref{app:ablation}.


\subsection{Inverse Autoregressive Training}
\label{sec:inverse-autoregressive-experiment}

As our first main experiment, we show that C-PTP can be trained directly from data. To this end, we follow the procedure in \cref{sec:autoregressive-training} and train a model from scratch on a dataset that predicts sequences of pick-up locations for taxis in New York City (NYC TLC, \citeyear{nyc_tlc_2016}).

\Cref{tab:perp} shows that the resulting model matches the performance of an autoregressive teacher. Based on the evidence for multi-token prediction (MTP) \citet{gloeckle2024better,deepseek-ai2025deepseekv3}, we expect PTP our model to outperform autoregressive baselines at larger scales. We leave training larger models from scratch to future work. Next, we show how PTP already outperforms MTP frameworks in terms of inference speed. 


\begin{table}[]
    \centering
    \begin{tabular}{cc}
        \toprule
         Model & Perplexity ($\downarrow$) \\
         \midrule
         C-PTP (Ours) & 19.88 \\
         Autoregressive & 19.81 \\
         \bottomrule
    \end{tabular}
    \caption{
    \textbf{C-PTP can be successfully trained from data alone.} The table shows that the perplexity of Categorical Parallel Token Prediction (C-PTP) and an autoregressive model on taxi pickup location sequences (NYC TLC, \citeyear{nyc_tlc_2016}) are almost identical.
    }
    \label{tab:perp}
\end{table}


\subsection{Limitations of Competing Frameworks}
\label{sec:competition-limitations}

Parallel Token Prediction overcomes important limitations in other frameworks to decrease latency: first, small surrogate models for speculative decoding do not leverage their parallel multi-token potential, which we unlock with our framework in \cref{sec:speculative-decoding}. Second, modeling tokens independently produces incoherent sequences (\cref{sec:independent-failure}). This also limits the representational power of discrete diffusion models \cite{hoogeboom2021argmax,austin2021structured}.
We compare to speculative decoding and independent token prediction on CodeContests \cite{li2023starcoder}, a dataset of coding challenges, by distilling \texttt{TinyLlama-1.1B-Chat-v1.0} \cite{zhang2024tinyllama}.


\subsubsection{Smaller Autoregressive Decoders}
\label{sec:speculative-decoding}

\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figs/speculative-comparison.pdf}
    \caption{
    Speculative decoding with Parallel Token Prediction on code.
    The x-axis shows the draft-model parameter count. The left panel reports wall-clock speedup relative to standard autoregressive decoding, and the right panel shows the average number of tokens accepted per step under teacher verification. Curves compare autoregressive draft models to Parallel Token Prediction (PTP) drafts; ($\bullet$) are models trained from scratch for a fixed number of epochs, and ($\star$) is finetuned from the teacher.
    Across model sizes, PTP drafts achieve higher speedups by generating more than one correct token per step, whereas autoregressive drafts remain sequential. \textbf{PTP allows parallelism in draft models, yielding larger speedups at equal model size.}
    }
    \label{fig:speculative-decoding}
\end{figure}

Speculative decoding distills small autoregressive drafting models to imitate the teacher with fewer parameters \cite{leviathan2023fast}. Through their reduced size, these models require less compute to predict each token, and the teacher is used to verify them in parallel as in \cref{sec:error-correction}. This results in an overall speedup.

\Cref{fig:speculative-decoding} shows that training these smaller draft networks with our parallel token prediction framework further reduces latency, as it requires only a single call to the student model generating several tokens in parallel.
The draft models for this experiment were trained to replicate \texttt{TinyLlama-1.1B-Chat-v1.0} \cite{zhang2024tinyllama} on CodeContests \cite{li2023starcoder}. They are trained from scratch for a fixed number of epochs based on completions by that teacher. Details in \cref{sec:experimental-details}.




\subsubsection{Independent Prediction}
\label{sec:independent-failure}

Many approaches to predicting multiple tokens in parallel, including multi-token prediction \cite{qi2020prophetnet,gloeckle2024better} and discrete diffusion models \cite{hoogeboom2021argmax,austin2021structured}, assume that future tokens are conditionally independent. As a result, later tokens are sampled from marginal distributions that average over incompatible earlier choices, making semantic and syntactic inconsistencies unavoidable even with infinite model capacity. In code generation, this manifests as spurious token combinations such as \texttt{def numpy} and \texttt{import find}. See \cref{app:independent-failure-formal} for a formal derivation. 

To isolate this effect, we compare models trained with informative auxiliary variables $u_i$ (our O-PTP) to models that have multiple independent heads for predicting multiple tokens~\cite{qi2020prophetnet,gloeckle2024better}.
As shown in \Cref{fig:independent-failure}, O-PTP consistently produces meaningful token combinations by coordinating predictions through the auxiliary variables, while independent prediction frequently produces incompatible pairs. Quantitatively, \cref{tab:codecontest} shows that auxiliary variables substantially increase the number of tokens that can be generated in a single model call. \Cref{fig:code-sampling} shows a qualitative sample of our model's predictions.


\begin{figure}[bht]
    \centering
    \includegraphics[width=\linewidth]{figs/independent-failure.pdf}
    \caption{
    \textbf{Parallel Token Prediction generates meaningful pairs of tokens.}
    Each panel plots the first and second auxiliary variables $(u_1,u_2)\in[0,1]^2$ on the axes; regions in this unit square correspond to the resulting two-token outputs (green: compatible, red: incompatible).
    \textit{(Left)} In a coding problem, autoregressive sampling first selects one of \texttt{def}, \texttt{import}, or \texttt{n}, and then continues with meaningful predictions: a function name to declare, a package to import, or a variable assignment.
    \textit{(Center)} Our code completion model produces similarly sensible token pairs, but in a single model call by coordinating predictions through the auxiliaries; only rarely ($<1\%$) does it yield spurious combinations like \texttt{def sys}.
    \textit{(Right)} A model that independently predicts future tokens is bound to fail: in about $60\%$ of cases, it combines incompatible tokens because the second token is not informed about the first.
    }
    \label{fig:independent-failure}
\end{figure}










\begin{table}
   \centering
   \begin{tabular}{cc}
       \toprule
       Parallelization technique & \accepted{} ($\uparrow$) \\
       \midrule
       O-PTP (ours) & \textbf{7.0 $\pm$ 0.1} \\
       Independent prediction & 6.2 $\pm$ 0.1 \\
       \bottomrule
   \end{tabular}
   \caption{\textbf{For code generation, speculative decoding with O-PTP accepts more tokens per step than independently predicting tokens.} We distill TinyLlama-1.1B \cite{zhang2024tinyllama} on coding problems \cite{li2022competitionlevel}, once with meaningful auxiliaries that determine token dependence and once with a generic \texttt{[MASK]} token. Error is difference between two runs.}
   \label{tab:codecontest}
\end{table}

\subsection{General-purpose Text Generation}
\label{sec:vicuna}


In the following experiment, we confirm that a pretrained model can be finetuned to predict several tokens by training it with the PTP framework.

To this end, we distill a One-Hot Parallel Token Prediction model (O-PTP) student from a strong autoregressive teacher, following the distillation procedure described in \cref{sec:distillation}. In our primary experiment, we use Vicuna-7B \cite{vicuna2023} as the teacher and fine-tune the student on ShareGPT conversational data \cite{chen2024sharegpt4v}. Instead of fine-tuning a full model we train a gated LoRA \cite{samragh2025your} adaptor, allowing us to make only the minimal changes that are necessary to correctly parse the auxiliary variables.

To reflect a practical deployment scenario for a chat-oriented large language model, we evaluate the performance on SpecBench \cite{xia2024unlocking}, containing a diverse set of tasks such as multi-turn conversation, translation, summarization, question answering, mathematical reasoning (GSM8K), and retrieval-augmented generation. Performance is measured using the average number of accepted tokens, as this is directly proportional to the maximum speedup that can be obtained with efficient decoding schemes.

As shown in \cref{tab:spec-bench}, O-PTP consistently predicts significantly more tokens identical to its teacher than existing parallelization baselines across most tasks. These results establish a new state of the art on SpecBench, demonstrating that PTP remains effective when scaled to larger models and heterogeneous real-world datasets.

\begin{table}[]
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        Parallelization & MTC & TL & SUM & QA & Math & RAG & Task-Average \\
        \midrule
        O-PTP (ours) & 4.52 & \textbf{3.73} & \textbf{4.54} & \textbf{3.42} & \textbf{4.62} & \textbf{4.29} & \textbf{4.18} \\
        SAMD \cite{hu2025sam} & \textbf{4.62} & 3.12 & 4.18 & 3.38 & 4.16 & 3.95 & 3.90 \\
        Eagle-2 \cite{li2024eagle} & 4.54 & 3.19 & 3.81 & 3.41 & 4.38 & 3.80 & 3.86 \\
        Hydra \cite{ankner2024hydra} & 3.90 & 2.88 & 2.95 & 3.21 & 3.90 & 3.37 & 3.37 \\
        Eagle \cite{li2024eagle1} & 3.66 & 2.74 & 3.15 & 2.81 & 3.57 & 3.21 & 3.19 \\
        Medusa \cite{cai2024medusa} & 2.70 & 2.18 & 2.12 & 2.24 & 2.67 & 2.26 & 2.36 \\
        SpS \cite{chen2023accelerating} & 2.12 & 1.39 & 2.16 & 1.80 & 1.92 & 2.26 & 1.94 \\
        Recycling \cite{luo2025turning} & 2.74 & 2.48 & 2.68 & 2.59 & 3.05 & 2.63 & 2.70 \\
        PLD \cite{saxena2023prompt} & 1.67 & 1.10 & 2.73 & 1.37 & 1.80 & 1.71 & 1.73 \\
        Lookahead \cite{fu2024break} & 1.69 & 1.24 & 1.54 & 1.56 & 1.92 & 1.42 & 1.56 \\
        None & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 & 1.00 \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Average number of accepted tokens under teacher verification ($\uparrow$).}
    For each method and task, we list the average length of the longest prefix matching the teacher’s output, which directly determines decoding speedup. We finetune a One-Hot Parallel Token Prediction (O-PTP) model by distilling Vicuna-7B~\cite{vicuna2023} on ShareGPT~\cite{chen2024sharegpt4v} and compare it to concurrent parallelization techniques on the diverse tasks of SpecBench~\cite{xia2024unlocking}, including Multi-turn Conversation (MTC), Translation (TL), Summarization (SUM), Question Answering (QA), Mathematical Reasoning (Math, GSM8K), and Retrieval-augmented Generation (RAG). Errors indicate the standard deviation over three runs. Temperature is set to $0.7$, and values are rounded to reflect statistical certainty. Across nearly all tasks, O-PTP achieves the highest number of accepted tokens.}
    \label{tab:spec-bench}
\end{table}

\section{Related Work}

Speeding up the generation of autoregressive models and discrete sequence models in particular has been the focus on a broad body of work, see \cite{khoshnoodi2024comprehensive} for an overview.

Our framework combines two ideas from the Normalizing Flow literature and imports them to modeling discrete data: Inverse Autoregressive Flows (IAF) are trained with fast prediction in mind \cite{kingma2016improved} by iteratively identifying latent variables (our auxiliary variables) that generate a particular continuous one-dimensional value, and Free-Form Flows (FFF) train a generating function when a fast parallel sampler is not available \cite{draxler2024freeform}.


In the LLM literature, speeding up generation has been approached from various angles. \textbf{Speculative decoding} takes a system perspective, using a small draft model to propose multiple tokens and a large target model to verify them \cite{leviathan2023fast,chen2023accelerating}. Variants verify entire sequences 
\cite{sun2023spectr} or use a smaller verifier network \citet{zhong2025speeding} to improve quality and speed. \textbf{Latent variable methods} first sample latent codes from the prompt so that the distribution of subsequent tokens factorizes given latent codes \cite{gu2018nonautoregressive,ma2019flowseq}.
\textbf{Diffusion language models} leave autoregressive sampling behind by iteratively refining the text starting from a noisy or masked variant \cite{hoogeboom2021argmax,austin2021structured}. \textbf{Multi-head output models} predict several next tokens independent of each other \cite{qi2020prophetnet,gloeckle2024better,deepseek-ai2025deepseekv3}, narrowing down on the possible set of next tokens. Both discrete diffusion and multi-head models assume independence of tokens, which is fundamentally limited in modeling capacity, compare \cref{sec:independent-failure} and \citet{feng2025theoretical}. Recent work on discrete diffusion models leverage copula models to capture dependencies in an additional model \cite{liu2025discrete}. We model dependencies between several tokens without an additional model at inference time.

In contrast to the above, our work introduces a new class of fast language models that are universal in the sense that can approximate arbitrary dependence between several tokens in a single model call. Our new method is complementary to existing approaches, and we leave exploring these combinations open for future research.


\section{Conclusion}


In this paper, we introduce \textit{Parallel Token Prediction}, a framework that permits consistent generation of several tokens in a single autoregressive model call. It eliminates the independence assumptions that limited prior approaches, allowing to model multiple tokens with arbitrary dependency between them. Empirically, we show that existing models can be distilled into efficient parallel samplers. With error correction, these models produce identical output as a teacher while significantly increasing how many tokens are obtained per model call.

This speedup makes language models more practical for real-time applications. Future work includes extending our framework to large scale models, multimodal generation, combining it with complementary acceleration strategies, and exploring theoretical limits on parallelization.

Overall, our results suggest that the sequential bottleneck in autoregressive transformers is not inherent, and that universal, efficient parallel generation is within reach.

We think that our experiments in \cref{sec:experiments} only scratch the surface of the possibilities enabled by our \cref{thm:one-hot-PTP,thm:C-PTP}. Replicating a teacher model limits the performance achievable by the student. We envision future work to train large models from scratch that think in long sequences, and conjecture that this will enable new reasoning capabilities.



\section*{Acknowledgments}

Sameer Singh acknowledges funding from the National Science Foundation (NSF) through an NSF CAREER award IIS-2046873. Stephan Mandt acknowledges funding from the National Science Foundation (NSF) through an NSF CAREER Award IIS-2047418, IIS-2007719, the NSF LEAP Center, and the Hasso Plattner Research Center at UCI, and the Chan Zuckerberg Initiative. This project was supported by the Chan Zuckerberg Initiative, and the Hasso Plattner Research Center at UCI.


\section*{Ethics Statement}
Our work focuses on reducing the inference time of Large Language Models, enabling more computations per unit time and supporting large-scale or real-time applications. While this can improve responsiveness and resource efficiency, it may also increase the potential for misuse, such as generating misinformation or automated spam at higher volumes. Faster inference does not mitigate underlying model biases, so responsible deployment, monitoring, and safeguards are critical to balance performance gains with societal risks.


\section*{Reproducibility Statement}
We include proofs for all theoretical results introduced in the main text in \cref{sec:proofs}. We include further experimental and implementation details (including model architectures and other hyperparameter choices) in \cref{sec:ablation} and \cref{sec:experimental-details}. Our code will be made available by the time of publication.


\bibliography{iclr2026_conference}
\bibliographystyle{iclr2026_conference}

\clearpage
\appendix


\section{Proofs}
\label{sec:proofs}

\subsection{Proof of \texorpdfstring{\cref{thm:one-hot-PTP}}{Theorem \ref{thm:one-hot-PTP}}}
\label{sec:proof-one-hot-PTP}

\begin{proof}
    By \cref{thm:C-PTP}, it holds that the distribution of token $t_{k}, k \geq i$ is fully determined by $t_{1}, \dots, t_{k-1}$ and $u_i, \dots, u_{k-1}$, showing that the categorical distribution $P_{k}$ of token $t_{k}$ is fully determined.

    Thus, the function to compute token $t_{k}$ is given by \cref{eq:autoregressive-sampling}:
    \begin{equation}
        t_{k} = f_P(t_1, \dots, t_{i-1}; u_i, \dots u_k) = \Pick(u_{k}; P_{k}).
    \end{equation}
\end{proof}

\subsection{Proof of \texorpdfstring{\cref{thm:C-PTP}}{Theorem \ref{thm:C-PTP}}}
\label{sec:proof-C-PTP}

\begin{proof}
    We prove by induction over $k, k\geq i$.

    For $k=i$, there is nothing to show, since there are no auxiliaries involved in the statement.


    For $k \mapsto k+1$, assume the statement holds for $k$. This gives us access to the distribution $P_{k}$ of the token $t_{k}$. Since token $t_{k}$ is uniquely determined from $P_{k}$ and $u_{k}$ via \cref{eq:deterministic-token-given-auxiliary}, any distribution conditioning on $P_{k}, t_{k}$ can instead condition on $P_{k}, u_{k}$ via the law of total probability.
\end{proof}


\section{Average number of correct tokens}
\label{accepted-tokens-metric}

To quantify the effectiveness of parallel token generation, we measure the number of correctly predicted tokens. Given a student output sequence $t_1^{\mathrm{S}}, t_2^{\mathrm{S}}, \dots, t_N^{\mathrm{S}}$ and a teacher output sequence $t_1^{\mathrm{T}}, t_2^{\mathrm{T}}, \dots, t_N^{\mathrm{T}}$ of the same length, this metric is defined as
\begin{align}
    \label{eq:correct-count}
    \correct &= \max \left\{ k \;\middle|\; t_j^{\mathrm{S}} = t_j^{\mathrm{T}} \;\; \forall\, j \le k \right\}. \\
    \accepted &= \min\{\correct + 1, N\}.
\end{align}
That is, \correct{} equals the length of the longest prefix of the student's output that exactly matches the teacher's output. Intuitively, this measures how many tokens the student can generate in parallel before the first disagreement, and directly corresponds to the number of tokens that can be safely accepted without error correction.

\section{Additional Ablation Results}
\label{app:ablation}

Based on a dataset (NYC TLC, ) that contains latitudes and longitudes for pick-up locations for all taxi rides in 2016, we divide the city into $25$ neighborhoods via $k$-Means clustering to obtain a discrete-valued time-series that we can split into overlapping chunks of length $N$. This is a common benchmark dataset in the literature of marked temporal point processes \cite{xue2024easytpp}, and autoregressive transformers are a common architecture \cite{draxler2025transformers}.


We now provide some of the specific choices we made when implementing the general framework of Parallel Token Prediction. Specifically, we discuss the empirical difference between O-PTP and C-PTP and which specific loss to choose. We will specify our model architecture and how to embed both tokens and auxiliary variables in the same embedding space, and lastly compare the proposal distributions our training sequences can be sampled from.

We test our framework by training a model that predicts pick-up locations for taxis in New York City. Based on a dataset (NYC TLC, \citeyear{nyc_tlc_2016}) that contains latitudes and longitudes for pick-up locations for all taxi rides in 2016, we divide the city into $25$ neighborhoods via $k$-Means clustering to obtain a discrete-valued time-series that we can split into overlapping chunks of length $N$. This is a common benchmark dataset in the literature of marked temporal point processes \cite{xue2024easytpp}.

As a teacher model, we pretrain a $29$M-parameter autoregressive causal transformer based on the architecture of GPT-2 \cite{radford2019language}, using the cross-entropy loss in \cref{eq:ce}. For our PLM we choose the same GPT-style transformer architecture as the teacher. This allows us to use the teacher's parameters as a warm-start.
We evaluate all our parallel models in terms of the average number of leading tokens predicted by our student model that are identical to the teacher. In the end, this is the quantity that limits the maximum latency reduction that can be achieved, see \cref{sec:error-correction}.

\subsection{Auxiliary Variable Embeddings}

In our experiments we use transformers that embedded tokens into a higher-dimensional embedding space via a learned embedding before adding a positional embedding. This doesn't work out-of-the box for our auxiliary variables since they are one-dimensional continuous variables. Thus we learn a separate embedding. We combine two components, for each of which we test several variants:
(1)~A learned affine linear transform [\textbf{lin}] or a fully connected neural network [\textbf{NN}]. (2)~Feed either the scalar $u$ [\textbf{fl}], a $n$-dimensional threshold-embedding $e_i = 1\{u \leq i/n\}$ [\textbf{th}], or an $n$-dimensional embedding $e_i = 1\{u 2^{i-1} \,\mathrm{mod}\, 1 \leq 0.5\}$ [\textbf{ar}] inspired by arithmetic coding \cite{witten1987arithmetic}.

Empirically, all methods work reasonably well, but a structured embedding leads to faster and more stable training convergence. This is similar to the transformer's positional embedding were both learned and fixed embeddings work well but the later is preferred in practice \cite{vaswani2017attention}. For further experiments we use the [\textbf{ar + lin}] embedding. \Cref{tab:u-embed} shows the detailed effect of different embedding strategies.


\subsection{Distillation Losses and Proposal Distributions}
\label{sec:losses}

As our framework is deliberately general, %
it is compatible with a wide selection of losses. We here compare the distillation losses (\cref{sec:distillation}), focusing on KL and cross-entropy losses in \cref{eq:o-ptp-cross-entropy,eq:c-ptp-divergence}. Specifically the KL loss (\textbf{kl}), reverse KL loss (\textbf{kl-rev}), binary cross-entropy loss (\textbf{bce}), and categorical cross-entropy loss (\textbf{ce}). During training we sample training sequences from a dataset and continuations $t_{\geq i}$ either from the teacher model $Q_\varphi$, the student model $P_\theta$, or directly from a dataset. \Cref{tab:losses} shows the results for different losses. Empirically we note, that O-PTPs are easier to train than C-PTPs and achieve a higher number of average correct tokens. This is most likely due to the fact that O-PTPs do not have to predict the full token distribution accurately, which includes tail behavior, as long as they learn which token is the most likely given the auxiliary variable. In the following, we choose to sample training sequences from the teacher model for best results.

\begin{table}[]
    \centering
    \begin{tabular}{c|cc|cc|c}
        \toprule
        Proposal Distribution $P(t)$ & \textbf{kl} & \textbf{kl-rev} & \textbf{bce} & \textbf{ce} & MTP\\
        \midrule
        Teacher & $40$ & $41$ & $\mathbf{45}$ & $44$ & $10.1$ \\
        Student & $44$ & $39$ & $\mathbf{45}$ & $\mathbf{45}$ & $10.1$ \\
        Dataset & $29$ & $36$ & $44$ & $43$ & $10.1$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Our framework is compatible with several losses.} Average number of correct tokens ($\uparrow$) on the taxi dataset, evaluated on 16000 samples. O-PTP are distilled with KL or reverse KL loss (\textbf{kl}, \textbf{kl-rev}), C-PTP with binary or categorical cross entropy loss (\textbf{bce}, \textbf{ce}). Independent prediction (MTP) \cite{gloeckle2024better} achieves $10.1$. Numbers rounded to reflect level of statistical certainty.}
    \label{tab:losses}
\end{table}


\begin{table}
    \centering
    \begin{tabular}{c|ccccc|c}
        \toprule
        Model & \textbf{fl} + \textbf{NN} & \textbf{th} + \textbf{lin} & \textbf{th} + \textbf{NN} & \textbf{ar} + \textbf{lin} & \textbf{ar} + \textbf{NN} & MTP \\
        \midrule
        O-PTP & $35.9$ & $40.9$ & $39.1$ & $45.4$ & $\mathbf{46.1}$ & $10.1$\\
        C-PTP & $28.8$ & $36.8$ & $36.6$ & $\mathbf{40.4}$ & $35.7$ & $10.1$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Structured embeddings of auxiliary variables $u_k$ are more stable than fully-learned embeddings.} Average number of correct tokens ($\uparrow$) on the taxi dataset, evaluated on 16000 samples. Trained using the KL loss (C-PTP) and binary cross-entropy loss (O-PTP), respectively. Independent prediction (MTP) \cite{gloeckle2024better} achieves $10.1$. Numbers rounded to reflect level of statistical certainty.}
    \label{tab:u-embed}
\end{table}

\section{Sampling of Auxiliary Variables}
\label{sec:beta}

Our framework conditions, for a prompt $t_{<i}$, not on token $t_{k}$ directly but on the auxiliary variable $u_{k} \in [F_{k, t_{k}-1}, F_{k, t_{k}})$ that contains the same information. During inference we sample $u_k \sim \mathcal{U}[0, 1]$ as to not bias our predictions. During training on the other hand, we have more flexibility and can sample the permissible interval using 
$u_{k} = F_{k, t_{k}-1} + \tilde{u}_{k} \left[F_{k, t_{k}} - F_{k, t_{k}-1}\right]$, where $\tilde{u}_{k} \sim \mathrm{Beta}(b, b)$. For $b=1$ this simplifies to a uniform distribution while $b\neq1$ puts more or less weight on predictions that land closer to the border of the permissible interval and thus are more difficult to predict. Training results for different values of $b$ can be found in \Cref{tab:beta}. Empirically, we find that while the choice of $b$ does not seem to effect the final average number of correct samples, a larger $b$ might speeds up the earlier stages of training while a smaller $b$ might yield slightly better sample quality during inference, as measured by model perplexity.

\begin{table}[]
    \centering
    \begin{tabular}{c|ccc|c}
        \toprule
        $b$ & $2$ & $1$ & $0.5$ & MTP \\
        \midrule
        O-PTP & $12.9$ & $\mathbf{13.9}$ & $13.8$ & $8.4$ \\
        C-PTP & $13.6$ & $\mathbf{13.8}$ & $13.5$ & $8.4$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Different sampling strategies for $u_k$ are available.} Average number of correct tokens ($\uparrow$) for $\tilde{u}_k \sim \mathrm{Beta}(b, b)$ on the taxi dataset, evaluated on 16000 samples, with $N=16$. Trained using the KL loss (C-PTP) and binary cross-entropy loss (O-PTP), respectively. Independent prediction (MTP) \cite{gloeckle2024better} achieves $8.4$. Numbers rounded to reflect level of statistical certainty.}
    \label{tab:beta}
\end{table}

\section{Abundant Computational Resources}
\label{sec:more_gpu}

If reducing latency is more important than total compute, we can already start predicting more tokens by another PTP call, for example, by prematurely accepting the first $c$ tokens, while the teacher has not yet verified the predicted sequence. If the first $c$ tokens are confirmed to be correct, we select the produced sequence and repeat the process. If we do this with many different offsets in parallel, we minimize the chances that none of the generated sequences will be selected, and can completely avoid the overhead of error correction. In practice, this may be implemented using multiple GPUs or in one GPU with appropriately constructed attention masks \cite{samragh2025your, li2024eagle, lin2025bita}.

Another way to leverage several models run at once is to use them to improve the expected number of correct tokens directly. Specifically, for a fixed context we can let $M$ PTPs compute $M$ independent predictions using independently drawn auxiliary variables $u_{i,m}, \dots, u_{i + N, m}$. By choosing the best prediction, i.e. the one that gives us the best chance of a higher number of correct tokens, we can improve latency further.

Crucially, we have to choose the best prediction in a way that doesn't bias the marginal distribution over future tokens. If we, for example, naively choose the sequence that is correct for the most amount of tokens, we will bias our prediction towards sequences that are easier to predict. On way to achieve bias-free improvements is to pick the set of auxiliary variables that lands, on average, closest to the center of a token's valid interval $I_k(t_k) = [F_{k, t_{k}}, F_{k, t_{k}-1})$ where $F_{k, t_{k}}$ is the cumulative probability under $Q_\varphi$ to choose $t_{k}$ when predicting that token. Specifically, choose
\begin{align}
    \label{eq:maxu}
    \mathrm{argmax}_m \sum_{k=i}^{i+N}\abs{\frac{u_{k,m} - F_{k, t_{k,m}}}{F_{k, t_{k.m}-1} - F_{k, t_{k,m}}} - \frac{1}{2}}.
\end{align}
This does not bias the marginal distribution but does bias the distribution of the selected $u_k$ to be closer to the center of its interval $I_k(t_k)$. making the prediction less prone to small differences in the teacher's and student's logits. In the limit $M \to \infty$ we always select the middle point of $I_k(t_k)$, yielding an upper bound to the possible improvement. \Cref{tab:more_gpus} shows the performance gains on the taxi dataset.

\begin{table}[]
    \centering
    \begin{tabular}{c|cccccccc}
        \toprule
        M & $1$ & $4$ & $16$ & $64$ & $256$ & $1024$ & $10^6$ & $\infty$ \\
        \midrule
        Avg. correct tokens & $45.36$ & $49.67$ & $54.76$ & $55.74$ & $56.91$ & $59.79$ & $65.42$ & $90.17$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Additional compute increases correctness}. Average number of correct tokens ($\uparrow$) for $M$ O-PTPs running in parallel on the taxi dataset, with sequence length $N=100$.}
    \label{tab:more_gpus}
\end{table}

We can combine both techniques, avoiding the additional latency of verification while still keeping the higher expected number of correct tokens. Because the selection in \cref{eq:maxu} relies on the teacher logits it can only be made after the verification step. To avoid waiting for the verification we assume, as before, that after a model call one of $n=1 \dots S$ tokens are correct and pre-compute the future tokens based on this assumption. Instead of one call as before, we now have to make $M$-many calls for each $n$. After the verification step we discard all but the best call from the correct $n^*$. As we have to repeat this for all $M$ viable calls, that are yet to being verified, in parallel this approach benefits from $M^2S$ PTPs running in parallel.

\section{Restricted Computational Resources}
\label{sec:smalln}
Limiting the number $N$ of token's our PTP predicts at once to a smaller number will reduce the total number of floating point operations, increasing energy efficiency. This, of course, negatively effects the possible latency gains, especially since $N$ is an upper bound on the average number of correct tokens. \Cref{tab:less_gpus} shows the result for different values of $N$ on the taxi dataset.


\begin{table}[h]
    \centering
    \begin{tabular}{c|cccccccc}
        \toprule
        N & $1$ & $2$ & $4$ & $8$ & $16$ & $64$ & $100$ & $\infty$ \\
        \midrule
        Avg. correct tokens & $1.00$ & $1.99$ & $3.93$ & $7.59$ & $13.90$ & $36.60$ & $45.36$ & $48.76$ \\
        MTP & $1.00$ & $1.91$ & $3.53$ & $5.86$ & $8.40$ & $10.08$ & $10.07$ & $10.20$ \\
        \bottomrule
    \end{tabular}
    \caption{\textbf{Less compute decreases correctness}. Average number of correct tokens ($\uparrow$) for limited number of predicted tokens $N$ per O-PTP call on the taxi dataset, $M=1$.}
    \label{tab:less_gpus}
\end{table}










\section{Formal limitations of independent prediction}
\label{app:independent-failure-formal}

Without the auxiliary variables, the distribution of all tokens $k > i + 1$ is not informed about the choice we make for tokens $t_{i+1}, \dots t_{k-1}$. Instead, they are marginalized out, which explains the spurious code snippets:
\begin{align}
    \text{Independent: }& P(t_{k}|t_{<i}) = \sum_{t_{i}, \dots t_{k-1}} P(t_{k}|t_{<k}) P(t_{i}, \dots, t_{k-1}|t_{<i}) \neq P(t_{k}|t_{<k}). \\
    \text{Dependent (ours): }& P(t_{k}|t_{<i}, u_i, \dots u_k) \overset{\text{thm.~\ref{thm:C-PTP}}}{=} P(t_{k}|t_{<k}).
\end{align}
This limits how many tokens can be sampled in parallel without auxiliary variables, even for infinite model capacity. Our \cref{thm:one-hot-PTP,thm:C-PTP} allow for coordinating tokens, making model capacity the only restriction.


\section{Experimental Details}
\label{sec:experimental-details}

\subsection{Algorithms}
\label{sec:algorithms}

\Cref{alg:ptp-distillation} shows how to distill a PTP from a teacher, \cref{alg:ptp-inverse-autoregressive} shows how to train directly from data.

\begin{algorithm}[h]
\caption{Sampling with error correction}
\label{alg:error-correction}
\begin{algorithmic}
\Require Prompt $t_{1}, \dots, t_{i}$, one-hot or categorical PTP $P_\theta$, verification model $Q_\varphi(t)$.
\State Sample $u_k \sim \Uu[0, 1]$ for all $k \geq i$.
\While{$i < T$}
\If{$P_\theta$ is one-hot PTP}
    \State $P_k \gets P_\theta(t_k|t_{<i}, u_i, \dots, u_{k})$, jointly for all $k \geq i$. \Comment{Student one-hot distributions}
    \State $t_k = \mathrm{argmax}_l P_{kl}$, for all $k \geq i$. \Comment{\cref{eq:o-ptp-sampling}}
\Else
    \State $P_k \gets P_\theta(t_k|t_{<i}, u_i, \dots, u_{k-1})$, jointly for all $k \geq i$. \Comment{Student categorical distributions}
    \State $t_k = \Pick(u_k, P_{k})$, for all $k \geq i$. \Comment{\cref{eq:c-ptp-sampling}}
\EndIf
\State $Q_k \gets Q_\varphi(t_k|t_{<k})$, jointly for all $k \geq i$. \Comment{Teacher categorical distributions}
\State $\tilde t_k \gets \Pick(u_k, Q_{k})$, for all $k \geq i$. \Comment{Teacher tokens}
\State $i \gets \min_{k > i} \{ k : t_k \neq \tilde t_k\} - 1$, or $N$ if no mistake was made. \Comment{Compare cf.~\cref{eq:correct-count}}
\If{$\correct < N$}
    \State $t_i \gets \tilde t_i$. \Comment{All together: $\accepted = \correct + 1$}
    \State $i \gets i + 1$
\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[h]
\caption{Training PTP (distillation)}
\label{alg:ptp-distillation}
\begin{algorithmic}
\Require Sequence proposal distribution $P(t)$ (teacher, student, dataset, or combination), cutoff distribution $P(i|t)$, teacher model $Q_\varphi(t)$, one-hot or categorical PTP $P_\theta$.
\While{not converged}
\State Sample $t \sim P(t)$
\State $P_k = Q_\varphi(t_k \mid t_{<k})$ in single model call.
\State Sample $u_k \in [F_{k,t_k-1}, F_{k,t_k})$.
\State Sample $i \sim P(i|t)$.
\State Compute $\nabla_\theta \mathcal{L}(\theta, t)$ using one of \cref{eq:o-ptp-cross-entropy,eq:c-ptp-cross-entropy,eq:c-ptp-divergence}.
\State Gradient step.
\EndWhile
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Training PTP (inverse autoregressive)}
\label{alg:ptp-inverse-autoregressive}
\begin{algorithmic}
\Require Dataset $P(t)$, cutoff distribution $P(i|t)$, categorical PTP $P_\theta$.
\While{not converged}
\State Sample training sequence $t_1, \dots, t_N$.
\State Sample split position $i \in \{1, \dots, N\}$.
\For{$k = i, \dots, N$}
\State $P_k = P_\theta(t_k \mid t_{<i}, u_{i}, \dots, u_{k-1})$, with auxiliary available form previous iterations.
\State Sample $u_k \in [F_{k,t_k-1}, F_{k,t_k})$.
\EndFor
\State Compute $\nabla_\theta \mathcal{L}(\theta, t)$ using \cref{eq:ce-iat}.
\State Gradient step.
\EndWhile
\end{algorithmic}
\end{algorithm}



\subsection{Training details}
\label{sec:training-details}

The model used in \cref{sec:inverse-autoregressive-experiment,app:ablation} is a GPT-2–style transformer language model with 4 transformer layers, a hidden size of 1536, and approximately 29 million trainable parameters. Each layer follows the standard GPT-2 architecture, consisting of multi-head self-attention and position-wise feedforward sublayers, combined with residual connections and layer normalization. The vocabulary size is set $25$. Unless otherwise noted, all other hyperparameters and initialization schemes follow the original GPT-2 specification \cite{radford2019language}. During training and inference of our student model we don't provide any context and evaluate the correctness of the next $N=100$ tokens, by comparing $Q_\varphi(t_k \mid t_{<k})$ and $P_\theta(t_k)$. For results on a smaller $N=16$, see \cref{sec:smalln}. We train every model for $150$k steps with a batch size of $32$ with the Adam optimizer \cite{kingma2015adam} and learning rate $0.0001$.

The teacher model used in \cref{sec:competition-limitations} is a dialogue-tuned variant of the TinyLlama \cite{zhang2024tinyllama} 1.1 billion parameter model, adopting the same architecture and tokenizer as LLaMA 2 \cite{touvron2023llama2}: \texttt{TinyLlama-1.1B-Chat-v1.0}. The model uses a transformer architecture comprising $22$ transformer layers, each with standard multi-head self-attention, SwiGLU feedforward blocks, residual connections, and layer normalization. The embedding and hidden dimension is $2048$, and the intermediate (feedforward) dimension is $5632$, consistent with a LLaMA-style scaling. The vocabulary size is $32,000$. The parameters are available via \url{https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0}.

We train an O-PTP on predicting 16 additional tokens. We adopt the strategy of finetuning using gated low-rank adaptation, where changed parameters are only applied for completion tokens \cite{samragh2025your}. We choose a LoRA \cite{hu2022lora} rank of $r=256$, although comparable speedups can be achieved already with $r=64$. We train every model for steps with a batch size of $56$ with the AdamW optimizer \cite{loshchilov2019decoupled} on \cref{eq:o-ptp-cross-entropy} and learning rate $0.0001$ for 1.3M steps. We generate training and validation data by generating Python code completions on CodeContests \cite{li2022competitionlevel} from the teacher, splitting generated sequences between randomly into input and completion. We use a teacher sampling temperature of 0.7, top-$k=50$ and top-$p=0.9$, as is recommended for this model.

For the MTP baseline, we use \cref{eq:c-ptp-cross-entropy} with uninformative $u$s in otherwise identical code for a fair comparison.

For the scaling experiments in \cref{sec:speculative-decoding}, we parameterize models of different scales as in \cref{tab:scaling-model-configs}. We adopt the same architecture choices as TinyLlama.

For the natural language tasks on SpecBench \cite{xia2024unlocking}, we finetune Vicuna-7B-v1.5~\cite{vicuna2023} on ShareGPT \cite{chen2024sharegpt4v}. We train it with the same gated $r=128$ LoRA setup as for TinyLlama, but with batch size $64$ and for 1M steps.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Model Size & Hidden Size & Hidden Layers & Attention Heads \\
\midrule
4k    & 16   & 1  & 1  \\
66k   & 64   & 1  & 1  \\
525k  & 128  & 2  & 2  \\
4.2M  & 256  & 4  & 4  \\
34M   & 512  & 8  & 8  \\
268M  & 1024 & 16 & 16 \\
1.1B  & 2048 & 22 & 32 \\
\bottomrule
\end{tabular}
\caption{Model configuration by parameter scale for scaling experiment}
\label{tab:scaling-model-configs}
\end{table}


We base our code on \texttt{transformers} \cite{wolf2019huggingface}, \texttt{PyTorch} \cite{paszke2019pytorch}, PyTorch Lightning \citep{falcon2019pytorch}, Numpy \citep{harris2020array}, Matplotlib \citep{hunter2007matplotlib} for plotting and Pandas \citep{mckinney2010data} for data evaluation.


\subsection{Prompt for \texorpdfstring{\Cref{fig:independent-failure}}{Figure \ref{fig:independent-failure}}}

\begin{lstlisting}[breaklines=true]
You are given a permutation p_1, p_2, ..., p_n.

In one move you can swap two adjacent values.

You want to perform a minimum number of moves, such that in the end there will exist a subsegment 1,2,..., k, in other words in the end there should be an integer i, 1 <= i <= n-k+1 such that p_i = 1, p_{i+1} = 2, ..., p_{i+k-1}=k.

Let f(k) be the minimum number of moves that you need to make a subsegment with values 1,2,...,k appear in the permutation.

You need to find f(1), f(2), ..., f(n).

Input

The first line of input contains one integer n (1 <= n <= 200 000): the number of elements in the permutation.

The next line of input contains n integers p_1, p_2, ..., p_n: given permutation (1 <= p_i <= n).

Output

Print n integers, the minimum number of moves that you need to make a subsegment with values 1,2,...,k appear in the permutation, for k=1, 2, ..., n.

Examples

Input


5
5 4 3 2 1


Output


0 1 3 6 10 


Input


3
1 2 3


Output


0 0 0
\end{lstlisting}




\end{document}
