
@inproceedings{gu2018nonautoregressive,
    title = {Non-autoregressive neural machine translation},
    url = {https://openreview.net/forum?id=B1l8BtlCb},
    booktitle = {International conference on learning representations},
    author = {Gu, Jiatao and Bradbury, James and Xiong, Caiming and Li, Victor O.K. and Socher, Richard},
    year = {2018},
}

@inproceedings{leviathan2023fast,
    series = {Proceedings of machine learning research},
    title = {Fast inference from transformers via speculative decoding},
    volume = {202},
    url = {https://proceedings.mlr.press/v202/leviathan23a.html},
    abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
    booktitle = {Proceedings of the 40th international conference on machine learning},
    publisher = {PMLR},
    author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
    month = jul,
    year = {2023},
    pages = {19274--19286},
}
@inproceedings{sun2023spectr,
    title = {{SpecTr}: {Fast} speculative decoding via optimal transport},
    volume = {36},
    url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6034a661584af6c28fd97a6f23e56c0a-Paper-Conference.pdf},
    booktitle = {Advances in neural information processing systems},
    publisher = {Curran Associates, Inc.},
    author = {Sun, Ziteng and Suresh, Ananda Theertha and Ro, Jae Hun and Beirami, Ahmad and Jain, Himanshu and Yu, Felix},
    editor = {Oh, A. and Naumann, T. and Globerson, A. and Saenko, K. and Hardt, M. and Levine, S.},
    year = {2023},
    pages = {30222--30242},
}
@inproceedings{zhong2025speeding,
    title = {Speeding up speculative decoding via sequential approximate verification},
    url = {https://openreview.net/forum?id=Y4KcfotBkf},
    booktitle = {{ES}-{FoMo} {III}: 3rd workshop on efficient systems for foundation models},
    author = {Zhong, Meiyu and Teku, Noel and Tandon, Ravi},
    year = {2025},
}
@article{chen2023accelerating,
    title = {Accelerating large language model decoding with speculative sampling},
    journal = {arXiv preprint arXiv:2302.01318},
    author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
    year = {2023},
}

@inproceedings{ma2019flowseq,
    address = {Hong Kong},
    title = {{FlowSeq}: {Non}-autoregressive conditional sequence generation with generative flow},
    booktitle = {Proceedings of the 2019 conference on empirical methods in natural language processing},
    author = {Ma, Xuezhe and Zhou, Chunting and Li, Xian and Neubig, Graham and Hovy, Eduard},
    month = nov,
    year = {2019},
}
@inproceedings{gloeckle2024better,
    title = {Better \& faster large language models via multi-token prediction},
    booktitle = {Proceedings of the 41st international conference on machine learning},
    author = {Gloeckle, Fabian and Idrissi, Badr Youbi and Rozière, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
    year = {2024},
    pages = {15706--15734},
}
@inproceedings{qi2020prophetnet,
    address = {Online},
    title = {{ProphetNet}: {Predicting} future n-gram for sequence-to-{SequencePre}-training},
    url = {https://aclanthology.org/2020.findings-emnlp.217/},
    doi = {10.18653/v1/2020.findings-emnlp.217},
    abstract = {This paper presents a new sequence-to-sequence pre-training model called ProphetNet, which introduces a novel self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of optimizing one-step-ahead prediction in the traditional sequence-to-sequence model, the ProphetNet is optimized by n-step ahead prediction that predicts the next n tokens simultaneously based on previous context tokens at each time step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large-scale dataset (160GB), respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new state-of-the-art results on all these datasets compared to the models using the same scale pre-training corpus.},
    booktitle = {Findings of the association for computational linguistics: {EMNLP} 2020},
    publisher = {Association for Computational Linguistics},
    author = {Qi, Weizhen and Yan, Yu and Gong, Yeyun and Liu, Dayiheng and Duan, Nan and Chen, Jiusheng and Zhang, Ruofei and Zhou, Ming},
    editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
    month = nov,
    year = {2020},
    pages = {2401--2410},
}
@misc{deepseek-ai2025deepseekv3,
    title = {{DeepSeek}-{V3} {Technical} {Report}},
    url = {http://arxiv.org/abs/2412.19437},
    doi = {10.48550/arXiv.2412.19437},
    abstract = {We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.},
    urldate = {2025-08-16},
    publisher = {arXiv},
    author = {DeepSeek-AI and Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Guo, Daya and Yang, Dejian and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Zhang, Haowei and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Li, Hui and Qu, Hui and Cai, J. L. and Liang, Jian and Guo, Jianzhong and Ni, Jiaqi and Li, Jiashi and Wang, Jiawei and Chen, Jin and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Song, Junxiao and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Xu, Lei and Xia, Leyi and Zhao, Liang and Wang, Litong and Zhang, Liyue and Li, Meng and Wang, Miaojun and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Mingming and Tian, Ning and Huang, Panpan and Wang, Peiyi and Zhang, Peng and Wang, Qiancheng and Zhu, Qihao and Chen, Qinyu and Du, Qiushi and Chen, R. J. and Jin, R. L. and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Xu, Runxin and Zhang, Ruoyu and Chen, Ruyi and Li, S. S. and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Wu, Shaoqing and Ye, Shengfeng and Ye, Shengfeng and Ma, Shirong and Wang, Shiyu and Zhou, Shuang and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Wang, T. and Yun, Tao and Pei, Tian and Sun, Tianyu and Xiao, W. L. and Zeng, Wangding and Zhao, Wanjia and An, Wei and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Li, X. Q. and Jin, Xiangyue and Wang, Xianzu and Bi, Xiao and Liu, Xiaodong and Wang, Xiaohan and Shen, Xiaojin and Chen, Xiaokang and Zhang, Xiaokang and Chen, Xiaosha and Nie, Xiaotao and Sun, Xiaowen and Wang, Xiaoxiang and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yu, Xingkai and Song, Xinnan and Shan, Xinxia and Zhou, Xinyi and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhu, Y. X. and Zhang, Yang and Xu, Yanhong and Xu, Yanhong and Huang, Yanping and Li, Yao and Zhao, Yao and Sun, Yaofeng and Li, Yaohui and Wang, Yaohui and Yu, Yi and Zheng, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Tang, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Wu, Yu and Ou, Yuan and Zhu, Yuchen and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Zha, Yukun and Xiong, Yunfan and Ma, Yunxian and Yan, Yuting and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Wu, Z. F. and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Huang, Zhen and Zhang, Zhen and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Gou, Zhibin and Ma, Zhicheng and Yan, Zhigang and Shao, Zhihong and Xu, Zhipeng and Wu, Zhiyu and Zhang, Zhongyu and Li, Zhuoshu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Gao, Ziyi and Pan, Zizheng},
    month = feb,
    year = {2025},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  journal={{OpenAI}},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{brown2020language,
    title = {Language models are few-shot learners},
    volume = {33},
    url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
    booktitle = {Advances in neural information processing systems},
    publisher = {Curran Associates, Inc.},
    author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M.F. and Lin, H.},
    year = {2020},
    pages = {1877--1901},
}
@misc{zhang2024tinyllama,
    title = {{TinyLlama}: {An} open-source small language model},
    author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
    year = {2024},
    note = {arXiv: 2401.02385 [cs.CL]},
}
@misc{cerebras2023slimpajama,
    title = {{SlimPajama}: {A} {627B} token cleaned and deduplicated version of {RedPajama}},
    url = {https://www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama},
    author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
    month = jun,
    year = {2023},
}
@article{ding2023enhancing,
    title = {Enhancing chat language models by scaling high-quality instructional conversations},
    journal = {arXiv preprint arXiv:2305.14233},
    author = {Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
    year = {2023},
}
@article{li2023starcoder,
    title = {{StarCoder}: {May} the source be with you!},
    journal = {Transactions on machine learning research},
    author = {Li, R and Allal, LB and Zi, Y and Muennighoff, N and Kocetkov, D and Mou, C and Marone, M and Akiki, C and Li, J and Chim, J and {others}},
    year = {2023},
    note = {Publisher: OpenReview},
}
@article{li2022competitionlevel,
    title = {Competition-level code generation with {AlphaCode}},
    volume = {378},
    url = {https://www.science.org/doi/abs/10.1126/science.abq1158},
    doi = {10.1126/science.abq1158},
    abstract = {Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3\% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.},
    number = {6624},
    journal = {Science},
    author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and de Masson d’Autume, Cyprien and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
    year = {2022},
    pages = {1092--1097},
}
@inproceedings{guo2017calibration,
    title = {On calibration of modern neural networks},
    booktitle = {International conference on machine learning},
    publisher = {PMLR},
    author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
    year = {2017},
    pages = {1321--1330},
}
@inproceedings{holtzman2020curious,
    title = {The curious case of neural text degeneration},
    booktitle = {International conference on learning representations},
    author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
    year = {2020},
}

@misc{nyc_tlc_2016,
    author       = {{New York City Taxi and Limousine Commission}},
    shortauthor   = {{NYC TLC}},
    title        = {2016 Yellow Taxi Trip Data},
    year         = {2017},
    note         = {City of New York, OpenData portal},
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Technical Report},
  year={2019}
}

@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Kingma, Diederik P. and Ba, Jimmy},
  journal={International Conference on Learning Representations (ICLR)},
  year={2015},
}

@inproceedings{loshchilov2019decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
}

@book{witten1987arithmetic,
  title={Arithmetic Coding for Data Compression},
  author={Witten, Ian H. and Neal, Radford M. and Cleary, John G.},
  year={1987},
  publisher={Communications of the ACM},
  volume={30},
  pages={520--540},
}
  %number={6},

@article{touvron2023llama2,
  title     = {LLaMA 2: Open Foundation and Fine-Tuned Chat Models},
  author    = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Daniel and Blecher, Lukas and Bogoychev, Nikolay and Brannon, William and Brohan, Anthony and Caballero, Humberto and Chadwick, Andy and Lee, Jenny and others},
  year      = {2023},
  journal   = {arXiv preprint arXiv:2307.09288}
}

@inproceedings{
hoogeboom2021argmax,
title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6nbpPqUCIi7}
}

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and Van Den Berg, Rianne},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}
@inproceedings{kingma2016improved,
    title = {Improved variational inference with inverse autoregressive flow},
    volume = {29},
    url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
    booktitle = {Advances in neural information processing systems},
    publisher = {Curran Associates, Inc.},
    author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
    editor = {Lee, D. and Sugiyama, M. and Luxburg, U. and Guyon, I. and Garnett, R.},
    year = {2016},
}
@inproceedings{draxler2024freeform,
    title = {Free-form {Flows}: {Make} {Any} {Architecture} a {Normalizing} {Flow}},
    booktitle = {Artificial {Intelligence} and {Statistics}},
    author = {Draxler, Felix and Sorrenson, Peter and Zimmermann, Lea and Rousselot, Armand and Köthe, Ullrich},
    year = {2024},
}
@inproceedings{xue2024easytpp,
    title = {{EasyTPP}: {Towards} open benchmarking temporal point processes},
    url = {https://arxiv.org/abs/2307.08097},
    booktitle = {International conference on learning representations ({ICLR})},
    author = {Xue, Siqiao and Shi, Xiaoming and Chu, Zhixuan and Wang, Yan and Hao, Hongyan and Zhou, Fan and Jiang, Caigao and Pan, Chen and Zhang, James Y. and Wen, Qingsong and Zhou, Jun and Mei, Hongyuan},
    year = {2024},
}
@misc{khoshnoodi2024comprehensive,
    title = {A {Comprehensive} {Survey} of {Accelerated} {Generation} {Techniques} in {Large} {Language} {Models}},
    url = {http://arxiv.org/abs/2405.13019},
    doi = {10.48550/arXiv.2405.13019},
    abstract = {Despite the crucial importance of accelerating text generation in large language models (LLMs) for efficiently producing content, the sequential nature of this process often leads to high inference latency, posing challenges for real-time applications. Various techniques have been proposed and developed to address these challenges and improve efficiency. This paper presents a comprehensive survey of accelerated generation techniques in autoregressive language models, aiming to understand the state-of-the-art methods and their applications. We categorize these techniques into several key areas: speculative decoding, early exiting mechanisms, and non-autoregressive methods. We discuss each category's underlying principles, advantages, limitations, and recent advancements. Through this survey, we aim to offer insights into the current landscape of techniques in LLMs and provide guidance for future research directions in this critical area of natural language processing.},
    urldate = {2025-09-25},
    publisher = {arXiv},
    author = {Khoshnoodi, Mahsa and Jain, Vinija and Gao, Mingye and Srikanth, Malavika and Chadha, Aman},
    month = may,
    year = {2024},
    note = {arXiv:2405.13019 [cs]},
    keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{paszke2019pytorch,
  title = {Pytorch: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  year = {2019}
}

@article{pati2017statistical,
  title = {On Statistical Optimality of Variational {{Bayes}}},
  author = {Pati, Debdeep and Bhattacharya, Anirban and Yang, Yun},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.08983},
  eprint = {1712.08983},
  archiveprefix = {arxiv}
}

@article{pearson1901lines,
  title = {On Lines and Planes of Closest Fit to Systems of Points in Space},
  author = {Pearson, Karl},
  year = {1901},
  month = nov,
  journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume = {2},
  number = {11},
  pages = {559--572},
  issn = {1941-5982, 1941-5990},
  doi = {10.1080/14786440109462720},
  urldate = {2024-01-10},
  langid = {english},
  file = {/home/felix/Zotero/storage/FBVF3ZBT/Pearson - 1901 - LIII. On lines and planes of closest fit to sys.pdf}
}

@article{peel2001fitting,
  title = {Fitting Mixtures of {{Kent}} Distributions to Aid in Joint Set Identification},
  author = {Peel, David and Whiten, William J and McLachlan, Geoffrey J},
  year = {2001},
  journal = {Journal of the American Statistical Association},
  volume = {96},
  number = {453},
  pages = {56--63},
  publisher = {Taylor \& Francis}
}

@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362}
}

@inproceedings{mckinney2010data,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  editor = {{van der Walt}, St{\'e}fan and {Jarrod Millman}},
  year = {2010}
}

@misc{falcon2019pytorch,
  title = {{{PyTorch}} Lightning},
  author = {Falcon, William and {The PyTorch Lightning team}},
  year = {2019},
  month = mar,
  copyright = {Apache-2.0},
  version = {1.4}
}

@article{hunter2007matplotlib,
  title = {Matplotlib: {{A 2D}} Graphics Environment},
  author = {Hunter, J. D.},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {90--95},
  abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.}
}


@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@inproceedings{
draxler2025transformers,
title={Transformers for Mixed-type Event Sequences},
author={Felix Draxler and Yang Meng and Kai Nelson and Lukas Laskowski and Yibo Yang and Theofanis Karaletsos and Stephan Mandt},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=MtwsRjPZhf}
}

@inproceedings{
liu2025discrete,
title={Discrete Copula Diffusion},
author={Anji Liu and Oliver Broadrick and Mathias Niepert and Guy Van den Broeck},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
url={https://openreview.net/forum?id=FXw0okNcOb}
}

@inproceedings{
feng2025theoretical,
title={Theoretical Benefit and Limitation of Diffusion Language Model},
author={Guhao Feng and Yihan Geng and Jian Guan and Wei Wu and Liwei Wang and Di He},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=fGBCRZQVse}
}

@inproceedings{oord2018parallel,
  title={Parallel wavenet: Fast high-fidelity speech synthesis},
  author={Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and others},
  booktitle={International conference on machine learning},
  pages={3918--3926},
  year={2018},
  organization={PMLR}
}

@article{samragh2025your,
  title={Your llm knows the future: Uncovering its multi-token prediction potential},
  author={Samragh, Mohammad and Kundu, Arnav and Harrison, David and Nishu, Kumari and Naik, Devang and Cho, Minsik and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2507.11851},
  year={2025}
}

@article{li2024eagle1,
  title={Eagle: Speculative sampling requires rethinking feature uncertainty},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2401.15077},
  year={2024}
}

@article{li2024eagle,
  title={Eagle-2: Faster inference of language models with dynamic draft trees},
  author={Li, Yuhui and Wei, Fangyun and Zhang, Chao and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2406.16858},
  year={2024}
}

@article{lin2025bita,
  title={BiTA: Bi-directional tuning for lossless acceleration in large language models},
  author={Lin, Feng and Yi, Hanling and Yang, Yifan and Li, Hongbin and Yu, Xiaotian and Lu, Guangming and Xiao, Rong},
  journal={Expert Systems with Applications},
  volume={279},
  pages={127305},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{hu2025sam,
  title={Sam decoding: Speculative decoding via suffix automaton},
  author={Hu, Yuxuan and Wang, Ke and Zhang, Xiaokang and Zhang, Fanjin and Li, Cuiping and Chen, Hong and Zhang, Jing},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={12187--12204},
  year={2025}
}

@article{ankner2024hydra,
  title={Hydra: Sequentially-dependent draft heads for medusa decoding},
  author={Ankner, Zachary and Parthasarathy, Rishab and Nrusimha, Aniruddha and Rinard, Christopher and Ragan-Kelley, Jonathan and Brandon, William},
  journal={arXiv preprint arXiv:2402.05109},
  year={2024}
}

@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}

@inproceedings{luo2025turning,
  title={Turning trash into treasure: Accelerating inference of large language models with token recycling},
  author={Luo, Xianzhen and Wang, Yixuan and Zhu, Qingfu and Zhang, Zhiming and Zhang, Xuanyu and Yang, Qing and Xu, Dongliang},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6816--6831},
  year={2025}
}

@inproceedings{he2024rest,
  title={Rest: Retrieval-based speculative decoding},
  author={He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason and He, Di},
  booktitle={Proceedings of the 2024 conference of the North American chapter of the association for computational linguistics: Human language technologies (volume 1: long papers)},
  pages={1582--1595},
  year={2024}
}

@article{fu2024break,
  title={Break the sequential dependency of llm inference using lookahead decoding},
  author={Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao},
  journal={arXiv preprint arXiv:2402.02057},
  year={2024}
}

@misc{saxena2023prompt,
    title = {Prompt Lookup Decoding},
    author = {Apoorv Saxena},
    year = {2023},
    month = {November},
    url = {https://github.com/apoorvumang/prompt-lookup-decoding/}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    url = {https://lmsys.org/blog/2023-03-30-vicuna/},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}

@inproceedings{chen2024sharegpt4v,
  title={Sharegpt4v: Improving large multi-modal models with better captions},
  author={Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
  booktitle={European Conference on Computer Vision},
  pages={370--387},
  year={2024},
  organization={Springer}
}

@article{xia2024unlocking,
  title={Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding},
  author={Xia, Heming and Yang, Zhe and Dong, Qingxiu and Wang, Peiyi and Li, Yongqi and Ge, Tao and Liu, Tianyu and Li, Wenjie and Sui, Zhifang},
  journal={arXiv preprint arXiv:2401.07851},
  year={2024}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}